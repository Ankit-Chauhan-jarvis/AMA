{
  "summary": {
    "SPEAKER_03": {
      "full_participation": 8,
      "non_participation": 0,
      "pretend_participation": 1,
      "total_utterances": 9,
      "utterances": [
        {
          "text": "Okay, we're live. Hello, everyone. Welcome to the Editor Extensions group meeting. It's December 7th. The first part here is an announcement from my side. So I've been asked what I think about hosting",
          "participation": "full_participation"
        },
        {
          "text": "I mean, I'm not sure if you're familiar with that. Awesome. Yeah, there was a time. I remember a job where I had at some point, I had three interns in parallel assigned to me and they were in. Hello. ",
          "participation": "full_participation"
        },
        {
          "text": "Awesome. Yes. Very happy about this. Yeah. And the process for becoming language server maintainer is extremely lean. So we were able to quick turn around there as well. Yeah. Nothing like the monolit",
          "participation": "full_participation"
        },
        {
          "text": "Um, yeah. So again, thank you everyone who's, who's been contributing to this, um, so far, and it looks like we have also made really good progress today on the cash. So kind of looking forward to thi",
          "participation": "full_participation"
        },
        {
          "text": "Yeah. So that was my question. I was like, I was actually surprised to learn that we have local copy. I didn't realize that was the case. And so it is the case then that we, there could be like, we do",
          "participation": "full_participation"
        },
        {
          "text": "this is a moment also to broadcast. By introducing the cache, the telemetry model has not changed, but we have basically two different, we're introducing two different sources of suggestions. One sour",
          "participation": "full_participation"
        },
        {
          "text": "don't know, Aaron having an epiphany, maybe?",
          "participation": "pretend_participation"
        },
        {
          "text": "Yeah. Cause like initially when I, it took me a while to find time to actually think this through. And then when I tried to think, it's through, I was like, I can't, I can't figure this out. I feel li",
          "participation": "full_participation"
        },
        {
          "text": "And I also felt like, I felt like there must be a good solution to this, but couldn't find it. So I was happy that others were good. Yeah. All right. Yeah. There's nothing else in the notes doc, any o",
          "participation": "full_participation"
        }
      ]
    },
    "SPEAKER_01": {
      "full_participation": 7,
      "non_participation": 0,
      "pretend_participation": 0,
      "total_utterances": 7,
      "utterances": [
        {
          "text": "Yeah, for sure. Well, in a previous role, I got to mentor like like half the interns sort of yearly, like all those sort of back under front end interns that you get to sort of speak with. There's loa",
          "participation": "full_participation"
        },
        {
          "text": "Yeah, for sure. And so I've first become a language server maintainer, I guess, over the weekend and sort of with some of the pushes to code. And then I've been able to get a lot of suggestions. There",
          "participation": "full_participation"
        },
        {
          "text": "Awesome. Cool. Yeah. And so, yeah, John, John and I had a pretty productive discussion around like automation. We could potentially do for visitor studio as well. Because right now, the diversity is p",
          "participation": "full_participation"
        },
        {
          "text": "Yeah, for sure. Uh, so essentially one, one of the things that came up this morning is we, we, uh, got through a review. We merged in changes for a language server caching. And so, uh, some after cach",
          "participation": "full_participation"
        },
        {
          "text": "Well, I know how to get points on a different thread that I've just thought of, uh, where right now we're doing an import to the file and it's committed into get, but instead of doing that, what we co",
          "participation": "full_participation"
        },
        {
          "text": "Yeah. So it's easy. So, yeah, I guess we can improve the way we're doing the poll. But right now it was a manual process to do the polling. I've got, I've, I'm going to add an MR job, which will fail ",
          "participation": "full_participation"
        },
        {
          "text": "Yeah. You joked about having renovates for Snowplow, but we could actually, that could literally solve the problem. Because what we would do is we just have, we just have a module that's available, an",
          "participation": "full_participation"
        }
      ]
    },
    "SPEAKER_02": {
      "full_participation": 2,
      "non_participation": 0,
      "pretend_participation": 0,
      "total_utterances": 2,
      "utterances": [
        {
          "text": "Yeah. So bear, I have, I have pulled in the latest language or version and have tested. So we're, we're good on visual studio as well.",
          "participation": "full_participation"
        },
        {
          "text": "Aaron, I'll, I'll reach out to you. I'd love to have a little sync call with you. I'll call real quick to make sure that we're on the same, same, same page.",
          "participation": "full_participation"
        }
      ]
    },
    "SPEAKER_04": {
      "full_participation": 1,
      "non_participation": 1,
      "pretend_participation": 2,
      "total_utterances": 4,
      "utterances": [
        {
          "text": "Yeah. Sounds good. All right. Awesome.",
          "participation": "pretend_participation"
        },
        {
          "text": "So we need to renovate for Snowblow, basically, right? Cool. Maybe",
          "participation": "full_participation"
        },
        {
          "text": "Yeah. All right, there's some more activity in the doc now, but I",
          "participation": "pretend_participation"
        },
        {
          "text": "All right.",
          "participation": "non_participation"
        }
      ]
    },
    "SPEAKER_00": {
      "full_participation": 1,
      "non_participation": 0,
      "pretend_participation": 1,
      "total_utterances": 2,
      "utterances": [
        {
          "text": "Is, uh, I was thinking like, is there a way to know that there is new schema published because, uh, I mean, like we, uh, we've done it manually, but we do not know, do not know when the new schema is ",
          "participation": "full_participation"
        },
        {
          "text": "Yeah. Because like say, we had now like say, um, we had a, we had a, we had a, we had a, we Uh, this schema was updated, but it didn't get the version change. That's why we get this like difference, l",
          "participation": "pretend_participation"
        }
      ]
    }
  },
  "detailed_results": [
    {
      "speaker": "SPEAKER_03",
      "text": "Okay, we're live. Hello, everyone. Welcome to the Editor Extensions group meeting. It's December 7th. The first part here is an announcement from my side. So I've been asked what I think about hosting an intern in our team next summer, intern or interns. I personally, when I was in IC, I really enjoyed mentoring interns back then. So I am excited by the idea. But in my experience, a big key to success for this is to make sure that there is like a clear and dedicated support for interns from like engineers on the team, like specifically like a engineer to one engineer to one intern kind of like mapping, you know, and that means that includes, you know, identifying the project to be done, training the intern on their tech skills, coaching, all of that. So if you are interested in mentoring an intern, please reach out to me, ideally today, just so I can let Darva know that there if there is interest or not in the team. This would be next summer. Yeah, next summer. I said that. Yeah. And I think it's like a couple months. And there are the program ran last year. So there's a lot of structure. Actually, I put a few links here to like the different issues like onboarding stuff, common concerns and how they're addressed and things like that. So there's there's quite some. Some stuff there as well. Yeah, there's some comments here. Aaron, do you want to do you want to verbalize?",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_01",
      "text": "Yeah, for sure. Well, in a previous role, I got to mentor like like half the interns sort of yearly, like all those sort of back under front end interns that you get to sort of speak with. There's loads of UX interns and stuff in the previous role as well and product management interns. I love that. So in the Belfast office, we sort of had like up to like 12 interns sort of initially. And then I was 12. I'm not going to say a bunch of interns. You know, like, you know, I was like, you know, I was like, you know, I was like, you know, like 12 to 15 and 20, and it increased from 20 even since I've left. And I really thought the program was great. It's really, really a good way to get in a bunch of junior engineers that was eager to learn and happy to jump on board and learn stuff.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_03",
      "text": "I mean, I'm not sure if you're familiar with that. Awesome. Yeah, there was a time. I remember a job where I had at some point, I had three interns in parallel assigned to me and they were in. Hello. Hey, Kai. The other guy. Hi. And we, one of them was in second year, one in third year and one fourth year of college. And the one in second year was actually the most, the most productive one that kind of blew it out of the water. So that was kind of an interesting experience as well. Anyway. All right. Sweet. So yeah, I'll, I'll, then I'll nominate at least Aaron or I'll let Darwin know Aaron, it sounds like you're interested. Other folks here, just like, yeah, John, you're saying you also have experience and enjoyed it. Let me know if you're actually interested to do it here. Yeah. Okay. Cool. All right. Awesome. And Dylan is also plus one for Insturn and slight, slight recency bias here, obviously. Or similarity, but I don't know. Both. Cool. Next up here, Aaron, you want to do the honors of the next announcement?",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_01",
      "text": "Yeah, for sure. And so I've first become a language server maintainer, I guess, over the weekend and sort of with some of the pushes to code. And then I've been able to get a lot of suggestions. There was sort of a lack of maintainers available and because I was already in the area anyways, I got moved from a reviewer to maintainer. So yeah, if anybody needs reviews, I'm happy to help now.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_03",
      "text": "Awesome. Yes. Very happy about this. Yeah. And the process for becoming language server maintainer is extremely lean. So we were able to quick turn around there as well. Yeah. Nothing like the monoliths really. All right. Sweet. Next one. Yeah. Discussion. So yeah, I wanted to touch a little bit about, you know, talk a bit about the latency or I guess it's not really just latency at this point, but the whole kind of sprint crunch stuff that's going on this week on code suggestions. So I think the main open extensions topics still are the language server cache. We have TreeSitter for VS code as well. And then there's formatting on JetBrains that Ali had started as well. So I just wanted to mention. Yeah. For anyone that doesn't have ongoing assignments related to any of this, like please contribute. The one thing I'd like people to contribute most to is to test the cache in VS code. Like just like try it out, leave feedback on that issue. Also, if it just works, just leave feedback that it works. That also kind of helped gain confidence that things work. Yeah. So thank you for that. Another point is like, this has been a long and stressful week. For some folks weeks, because. Some people did work over the weekend. So I want to remind everyone to like, you know, take breaks and to reach out to me if you're feeling overwhelmed as well, because this is not normal circumstances. I had this morning off for like an embassy appointment, which was something long planned. And I did it. It was nice to walk in the snow outside and breathe fresh air. So encourage everyone to walk wherever you are. Yeah. Aaron, you have the next point here. New of him. Also pulled in the latest language. Awesome. Thank you for that. Yeah. Oh yeah. I guess John is also the thing. Like, I don't know. I haven't kept mine in check, but it's good to make sure that whatever the latest is in the language, if it's still worse with visual studio, that'd be great. So yeah.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_02",
      "text": "Yeah. So bear, I have, I have pulled in the latest language or version and have tested. So we're, we're good on visual studio as well.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_01",
      "text": "Awesome. Cool. Yeah. And so, yeah, John, John and I had a pretty productive discussion around like automation. We could potentially do for visitor studio as well. Because right now, the diversity is pulling in the, like the windows binary. So I think we got a package, Jason, so we can get renovated to automate the merge requests. That would be really quick. So that way John doesn't have to go and manually do all the steps. John can just sort of sign off after doing sort of some manual testing and then eventually other team members. Once we know what the sort of steps are to do that task for visual studio, we can jump on, do the same. I'm assuming we have windows environments, which I don't.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_02",
      "text": "Aaron, I'll, I'll reach out to you. I'd love to have a little sync call with you. I'll call real quick to make sure that we're on the same, same, same page.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_04",
      "text": "Yeah. Sounds good. All right. Awesome.",
      "participation": "pretend_participation"
    },
    {
      "speaker": "SPEAKER_03",
      "text": "Um, yeah. So again, thank you everyone who's, who's been contributing to this, um, so far, and it looks like we have also made really good progress today on the cash. So kind of looking forward to this one as well. Um, all right. The next, next point here. So, okay. The next one, I didn't write much here. I just wrote language server in jet brains. Um, because with this week, basically we've also kind of been adding. So, so I feel like this week has also stolen the thunder a bit about, you know, you know, us getting the language server in vs code because that happened like on Friday last week. And I was like, yes, finally we have it, you know, it's great. And then, and then now all of this, and like, we didn't really have the time to pause and really, you know, be like, hooray. Like they did it. Like, yeah. We got the language server in vs code. Um, but we did and it's awesome. And I think everyone now is convinced that it is awesome and there's more and more stuff going in there. And so I've been thinking, yeah, I've been thinking more about this. And I think if, uh, I think on the gender inside, there is kind of still work, like there's work to be done in terms of this week's effort. Um, but I feel like on visual studio, for example, there isn't too much. So I was wondering like, if I should like, I don't know. Ask John to just start spiking this real quick or something, or like, kind of like look for options, dig a little bit more. Um, because I feel like we could, we could use this sooner rather than later. Um, yeah. So that's kind of like, just want to put this out there basically, um, similar approach to what we have in visual studio. Right. So, um, do it ourselves kind of approach rather than rely on the, on the ID provided, um, support because we can't do that. It's behind, uh, The paid it's for the paid editions of jet brains only. So, yeah, cool. Uh, yeah. John sounds like you're happy to do that. I mean, we also have our one -on -one right after this, we can discuss a bit more, see what, what, what, what could be a reasonable first stab at this. Um, yeah. All right. So yeah, this is a little bit me. Like, I feel like this. Yeah. Again, to be, to be, to be transparent. Like this week has kind of been a little bit more. I mean, we've kind of thrown all the plans and parties a little bit up in the air and then they landed again somewhere else. So I'm like, Hey, if we're throwing priorities in the air, I might as well throw this one also a little bit and see where it lands. So this is kind of like guerrilla, um, guerrilla prioritization a little bit. Cool. Uh, Aaron, you have the next point.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_01",
      "text": "Yeah, for sure. Uh, so essentially one, one of the things that came up this morning is we, we, uh, got through a review. We merged in changes for a language server caching. And so, uh, some after caching, there was some sort of, uh, we weren't doing telemetry quite yet. So we had disabled it and then we enabled it with a new field for tracking. And that field that we added was a breaking change. Um, and there's like a local copy of the schema file. And so we, uh, we, uh, a contributor came in, added in the changes to, uh, to that file, which were the correct changes, but it had to happen in an upstream revolts rain because it didn't. We, if we release a language server, we would have been sending. Uh, events that could have broke. Um, so I've gone ahead and tested with the latest language over and everything, uh, and add a sort of an MR to reverts initially, like just the like breaking part of the event, but still keeping the caching stuff. And I guess we have a lot of unit testing that that's pretty extensive and works well against the local version of the file, but we were not doing anything to make sure that that local version of file was reflective of what was upstream in the igloo repository for snowplow. And so I've already started. Uh, I've, I've made an MR or a log. I'll submit the MR after, but I've got the local changes right now to actually go and download the file in the MR pipeline. And so it will download the file from the igloo registry. And then once it's done that, if there's any get difference between the file and what's there, it will, it will just spit out the diff saying, Hey, there's a difference in the file. Make sure that you update with the local registry and commit the changes. And then after that, that will at least have our unit tests running against the correct. Yeah. So that's a good point. Um, I think it's, it's a good point. I think we need to really move to using snowplow micro NCI. So that we're testing that we're actually able to push the metrics up. But our unit testing just validates that the ski like our JSON parsing works. Not that the server will accept the JSON that we send up.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_03",
      "text": "Yeah. So that was my question. I was like, I was actually surprised to learn that we have local copy. I didn't realize that was the case. And so it is the case then that we, there could be like, we don't have a local copy. We don't have to have it. It's just, that's how it is right now. But we can, you mentioned using micro, which I don't know what that is, but it sounds like a snowplow thing. So we can sort of just pull, uh, we could pull the latest, like we don't, we wouldn't have to version the file in the language server anymore. Is that right? Or yeah.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_01",
      "text": "Well, I know how to get points on a different thread that I've just thought of, uh, where right now we're doing an import to the file and it's committed into get, but instead of doing that, what we could do is we could keep the import the same way and the import with ES bundle. We can actually download the file during build time to pull in the, the Jason schema. And so that could pull it from the actual registry. And so that way you had to be a build time failure if the file didn't exist in, and then we'd be using the file exactly as it was in the registry at build time, at least. So that's one way to go about it. But yeah, snowplow micro is running a snowplow, uh, locally. So whenever we're doing snowplow tracking, we can send it to our local registry instead of the GitLab. So we can send it to the Docker server for the telemetry. And so when you're sending it to that local Docker container in the service, you can test the things in a way that because the local snowplow micro will also pull the, from the production registry to check that all the metrics are good and still probably do additional validation that you can't get. Um, so still plot Michael will actually give a list of like bad, bad events that it received. And so we can literally end the job, run our whole suite of integration tests. And if there's any bad events, we can. Error on the build and say, Hey, this bad events was received by snowplow.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_00",
      "text": "Is, uh, I was thinking like, is there a way to know that there is new schema published because, uh, I mean, like we, uh, we've done it manually, but we do not know, do not know when the new schema is published in this like equal repository. I'm not sure how it's called.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_01",
      "text": "Yeah. So it's easy. So, yeah, I guess we can improve the way we're doing the poll. But right now it was a manual process to do the polling. I've got, I've, I'm going to add an MR job, which will fail your MR. If there's been a change since the last time it's been merged to main, but the, yeah, we need, we need some way to push, like if a new glue thing is there, get like do something we can maybe do a nightly job even where we run the lint job nightly. And if it fails on main, then we got to like get email. That would even, that would be a small iteration. That'd be an improvement.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_00",
      "text": "Yeah. Because like say, we had now like say, um, we had a, we had a, we had a, we had a, we Uh, this schema was updated, but it didn't get the version change. That's why we get this like difference, like our local schema was the same version. That's like equal sigma, but it was like, had different, like, I don't know, defined. Uh, so ideally this wouldn't happen. Uh, she shouldn't happen because they update those schema with meaning and use schema files. Right. And like, this is probably something that we need. Uh, to tracks that like schema was updated. Okay. Yeah, but yeah, I agree. This manual thing is not good.",
      "participation": "pretend_participation"
    },
    {
      "speaker": "SPEAKER_04",
      "text": "So we need to renovate for Snowblow, basically, right? Cool. Maybe",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_03",
      "text": "this is a moment also to broadcast. By introducing the cache, the telemetry model has not changed, but we have basically two different, we're introducing two different sources of suggestions. One source is the network, like the usual one, basically, like the AI gateway, and the other one is the cache. So as far as the clients are concerned, it's like they request a suggestion, and then the language server decides whether it'll serve it from the cache or it'll serve it from, you know, fetching it in the server, but it should be transparent to clients, which is why we can actually reuse the current model, telemetry model, without making changes, which is quite neat.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_04",
      "text": "Yeah. All right, there's some more activity in the doc now, but I",
      "participation": "pretend_participation"
    },
    {
      "speaker": "SPEAKER_03",
      "text": "don't know, Aaron having an epiphany, maybe?",
      "participation": "pretend_participation"
    },
    {
      "speaker": "SPEAKER_01",
      "text": "Yeah. You joked about having renovates for Snowplow, but we could actually, that could literally solve the problem. Because what we would do is we just have, we just have a module that's available, and that module would just build, we can have it where it like builds often or something like that, or it even like is a fork of the existing Snowplow project. And essentially if there's an update there, it rebuilds a node module, which has the JSONs available. And if that node module is updated, then it actually would trigger the regular, renovated project. So it's like a process. I think that would go in really, like that would fit really well with our existing, what we do for dependencies in general. It like sort of match that model, which would be good. So, but yes, I'm super happy about telemetry in general. I'm glad to see that we've pushed and come into an agreement. It was really active discussion. It's really good to see so many folks swarming around the sort of like, how do we get it to fit our existing model principle?",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_03",
      "text": "Yeah. Cause like initially when I, it took me a while to find time to actually think this through. And then when I tried to think, it's through, I was like, I can't, I can't figure this out. I feel like we need to change the model. And then I talked with Shakur and he was like, yeah, I think we need to change the model or we need to just not have to limit you. Or we, we talked about a few options and then busty is basically kind of cracked that he was like, wait, defer like the model is fine. Like don't change it. Just. And for me in my head, it's just, I was really like, yeah, requested loaded, like requested for me, man, just like a network request. There was no other way. It doesn't have to mean that though, but it was so, I don't know, branded in my head. I was just like, yeah, it doesn't work for when you introduce a cash, how could this work? But actually it can work. Like it's fine. It's just requested, you know, from the language server, which is, yeah. So this is cool.",
      "participation": "full_participation"
    },
    {
      "speaker": "SPEAKER_04",
      "text": "All right.",
      "participation": "non_participation"
    },
    {
      "speaker": "SPEAKER_03",
      "text": "And I also felt like, I felt like there must be a good solution to this, but couldn't find it. So I was happy that others were good. Yeah. All right. Yeah. There's nothing else in the notes doc, any other topics that people want to bring up and discuss.",
      "participation": "full_participation"
    }
  ]
}